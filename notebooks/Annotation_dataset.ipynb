{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the annotation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the steps involved in gathering, cleaning, and merging all manual annotations made by various analysist from various groups and using different software in a single large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotations are for each datasets are loaded, sorted, and re-writen into a parquet file. First, ecosound and the other linbraries need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "import os\n",
    "from ecosound.core.annotation import Annotation\n",
    "from ecosound.core.metadata import DeploymentInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: DFO - Snake Island RCA-In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of all the paths of all folders with the raw annotation and audio files for this deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\PhD\\Projects\\Dectector\\datasets\\DFO_snake-island_rca-in_20181017'\n",
    "deployment_file = r'deployment_info.csv' \n",
    "annotation_dir = r'manual_annotations'\n",
    "data_dir = r'audio_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a DeploymentInfo object to handle metadata for the deployment, and create an empty deployment info file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "Deployment = DeploymentInfo()\n",
    "\n",
    "# write empty file to fill in (do once only)\n",
    "Deployment.write_template(os.path.join(root_dir, deployment_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A csv file \"deployment_info.csv\" has now been created in the root_dir. It is empty and only has column headers, and includes teh following fiilds:\n",
    "\n",
    "* audio_channel_number\n",
    "* UTC_offset\n",
    "* sampling_frequency (in Hz)\n",
    "* bit_depth \n",
    "* mooring_platform_name\n",
    "* recorder_type\n",
    "* recorder_SN\n",
    "* hydrophone_model\n",
    "* hydrophone_SN\n",
    "* hydrophone_depth\n",
    "* location_name\n",
    "* location_lat\n",
    "* location_lon\n",
    "* location_water_depth\n",
    "* deployment_ID\n",
    "* deployment_date\n",
    "* recovery_date\n",
    "\n",
    "This file needs to be filled in by the user with teh appropriate deployment information. Once fileld in, the file can be loaded using the Deployment object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_channel_number</th>\n",
       "      <th>UTC_offset</th>\n",
       "      <th>sampling_frequency</th>\n",
       "      <th>bit_depth</th>\n",
       "      <th>mooring_platform_name</th>\n",
       "      <th>recorder_type</th>\n",
       "      <th>recorder_SN</th>\n",
       "      <th>hydrophone_model</th>\n",
       "      <th>hydrophone_SN</th>\n",
       "      <th>hydrophone_depth</th>\n",
       "      <th>location_name</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>location_lon</th>\n",
       "      <th>location_water_depth</th>\n",
       "      <th>deployment_ID</th>\n",
       "      <th>deployment_date</th>\n",
       "      <th>recovery_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-8</td>\n",
       "      <td>48000</td>\n",
       "      <td>24</td>\n",
       "      <td>bottom weight</td>\n",
       "      <td>SoundTrap 300</td>\n",
       "      <td>67674121</td>\n",
       "      <td>SoundTrap 300</td>\n",
       "      <td>67674121</td>\n",
       "      <td>13.4</td>\n",
       "      <td>Snake Island RCA-In</td>\n",
       "      <td>49.211667</td>\n",
       "      <td>-123.88405</td>\n",
       "      <td>13.4</td>\n",
       "      <td>SI-RCAIn-20181017</td>\n",
       "      <td>20181016T103806</td>\n",
       "      <td>20181203T120816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   audio_channel_number  UTC_offset  sampling_frequency  bit_depth  \\\n",
       "0                     1          -8               48000         24   \n",
       "\n",
       "  mooring_platform_name  recorder_type  recorder_SN hydrophone_model  \\\n",
       "0         bottom weight  SoundTrap 300     67674121    SoundTrap 300   \n",
       "\n",
       "   hydrophone_SN  hydrophone_depth        location_name  location_lat  \\\n",
       "0       67674121              13.4  Snake Island RCA-In     49.211667   \n",
       "\n",
       "   location_lon  location_water_depth      deployment_ID  deployment_date  \\\n",
       "0    -123.88405                  13.4  SI-RCAIn-20181017  20181016T103806   \n",
       "\n",
       "     recovery_date  \n",
       "0  20181203T120816  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load deployment file\n",
    "deployment_info = Deployment.read(os.path.join(root_dir, deployment_file))\n",
    "deployment_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the manual annotations for this dataset. Here annotatiosn were performed with Raven:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 annotation files found.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "13016 annotations imported.\n"
     ]
    }
   ],
   "source": [
    "# load all annotations\n",
    "annot = Annotation()\n",
    "annot.from_raven(os.path.join(root_dir, annotation_dir),\n",
    "                 class_header='Class',\n",
    "                 subclass_header='Sound type',\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fill in all the missing information in teh annotations field with the deployment information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fill in missing information\n",
    "annot.insert_values(software_version='1.5',\n",
    "                    operator_name='Stephanie Archer',\n",
    "                    audio_channel=deployment_info.audio_channel_number[0],\n",
    "                    UTC_offset=deployment_info.UTC_offset[0],\n",
    "                    audio_file_dir=os.path.join(root_dir, data_dir),\n",
    "                    audio_sampling_frequency=deployment_info.sampling_frequency[0],\n",
    "                    audio_bit_depth=deployment_info.bit_depth[0],\n",
    "                    mooring_platform_name=deployment_info.mooring_platform_name[0],\n",
    "                    recorder_type=deployment_info.recorder_type[0],\n",
    "                    recorder_SN=deployment_info.recorder_SN[0],\n",
    "                    hydrophone_model=deployment_info.hydrophone_model[0],\n",
    "                    hydrophone_SN=deployment_info.hydrophone_SN[0],\n",
    "                    hydrophone_depth=deployment_info.hydrophone_depth[0],\n",
    "                    location_name = deployment_info.location_name[0],\n",
    "                    location_lat = deployment_info.location_lat[0],\n",
    "                    location_lon = deployment_info.location_lon[0],\n",
    "                    location_water_depth = deployment_info.location_water_depth[0],\n",
    "                    deployment_ID=deployment_info.deployment_ID[0],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different annotation labels that were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unkown_invert', 'unknown_invert', 'fish', 'fish?', 'unknown', 'unkown', nan, 'whale?', '?', 'sea lion?', 'airplane', 'mammal']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there are some inconsistencies in the label names (e.g. 'unknown', 'unkown'). Let's rename the class labels so everything has a consistent same name. We'll use teh following convention:\n",
    "* 'FS' for fish\n",
    "* 'UN' for unknown sound\n",
    "* 'KW' for killer whale\n",
    "* 'ANT' for anthropogenic sound\n",
    "* 'HS' for harbor seal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.data['label_class'].replace(to_replace=['fish'], value='FS', inplace=True)\n",
    "annot.data['label_class'].replace(to_replace=['fish?','unkown_invert','unknown_invert','fish?','unknown','unkown','whale?','?','sea lion?','mammal'], value='UN', inplace=True)\n",
    "annot.data['label_class'].replace(to_replace=['airplane'], value='ANT', inplace=True)\n",
    "annot.data['label_class'].dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the class label are now all consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UN', 'FS', 'ANT']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having a look a summary of all the annotations available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class        ANT     FS   UN  Total\n",
      "deployment_ID                            \n",
      "SI-RCAIn-20181017    2  12337  672  13011\n",
      "Total                2  12337  672  13011\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the metadata (deployment information) are filled in the annotation fields and that all labels have been \"cleaned up\", we can save the dataset as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.to_parquet(os.path.join(root_dir, 'Annotations_dataset_' + deployment_info.deployment_ID[0] + '.parquet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can also be save as a Raven or PAMlab annotation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.to_pamlab(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +' annotations.log', single_file=True)\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: DFO - Snake Island RCA-Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat the step above for all the other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 annotation files found.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "1932 annotations imported.\n"
     ]
    }
   ],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\PhD\\Projects\\Dectector\\datasets\\DFO_snake-island_rca-out_20181015'\n",
    "deployment_file = r'deployment_info.csv' \n",
    "annotation_dir = r'manual_annotations'\n",
    "data_dir = r'audio_data'\n",
    "\n",
    "# Instantiate\n",
    "Deployment = DeploymentInfo()\n",
    "\n",
    "# write empty file to fill in (do once only)\n",
    "#Deployment.write_template(os.path.join(root_dir, deployment_file))\n",
    "\n",
    "# load deployment file\n",
    "deployment_info = Deployment.read(os.path.join(root_dir, deployment_file))\n",
    "\n",
    "# load all annotations\n",
    "annot = Annotation()\n",
    "annot.from_raven(os.path.join(root_dir, annotation_dir),\n",
    "                  class_header='Class',\n",
    "                  subclass_header='Sound type',\n",
    "                  verbose=True)\n",
    "\n",
    "# Manually fill in missing information\n",
    "annot.insert_values(software_version='1.5',\n",
    "                    operator_name='Stephanie Archer',\n",
    "                    audio_channel=deployment_info.audio_channel_number[0],\n",
    "                    UTC_offset=deployment_info.UTC_offset[0],\n",
    "                    audio_file_dir=os.path.join(root_dir, data_dir),\n",
    "                    audio_sampling_frequency=deployment_info.sampling_frequency[0],\n",
    "                    audio_bit_depth=deployment_info.bit_depth[0],\n",
    "                    mooring_platform_name=deployment_info.mooring_platform_name[0],\n",
    "                    recorder_type=deployment_info.recorder_type[0],\n",
    "                    recorder_SN=deployment_info.recorder_SN[0],\n",
    "                    hydrophone_model=deployment_info.hydrophone_model[0],\n",
    "                    hydrophone_SN=deployment_info.hydrophone_SN[0],\n",
    "                    hydrophone_depth=deployment_info.hydrophone_depth[0],\n",
    "                    location_name = deployment_info.location_name[0],\n",
    "                    location_lat = deployment_info.location_lat[0],\n",
    "                    location_lon = deployment_info.location_lon[0],\n",
    "                    location_water_depth = deployment_info.location_water_depth[0],\n",
    "                    deployment_ID=deployment_info.deployment_ID[0],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some inconsistent class labels here as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fish', 'fish?', 'fush', '?']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing labels according to our naming convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FS', 'UN']\n"
     ]
    }
   ],
   "source": [
    "annot.data['label_class'].replace(to_replace=['fish','fish?','fush'], value='FS', inplace=True)\n",
    "annot.data['label_class'].replace(to_replace=['?'], value='UN', inplace=True)\n",
    "annot.data['label_class'].dropna(axis=0, inplace=True)\n",
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class           FS  UN  Total\n",
      "deployment_ID                      \n",
      "SI-RCAOut-20181015  1909  23   1932\n",
      "Total               1909  23   1932\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the cleaned up dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as parquet file\n",
    "annot.to_parquet(os.path.join(root_dir, 'Annotations_dataset_' + deployment_info.deployment_ID[0] + '.parquet'))\n",
    "annot.to_pamlab(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +' annotations.log', single_file=True)\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3: ONC - Delta Node 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the same steps as teh prvious dataset. The difference here is that the annotations were performed with PAMlab instead of Raven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 annotation files found.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "857 annotations imported.\n"
     ]
    }
   ],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\PhD\\Projects\\Dectector\\datasets\\ONC_delta-node_2014'\n",
    "deployment_file = r'deployment_info.csv' \n",
    "annotation_dir = r'manual_annotations'\n",
    "data_dir = r'audio_data'\n",
    "\n",
    "# Instantiate\n",
    "Deployment = DeploymentInfo()\n",
    "\n",
    "# write empty file to fill in (do once only)\n",
    "#Deployment.write_template(os.path.join(root_dir, deployment_file))\n",
    "\n",
    "# # load deployment file\n",
    "deployment_info = Deployment.read(os.path.join(root_dir, deployment_file))\n",
    "\n",
    "# # load all annotations\n",
    "annot = Annotation()\n",
    "annot.from_pamlab(os.path.join(root_dir, annotation_dir), verbose=True)\n",
    "\n",
    "# Mnaually fill in missing information\n",
    "annot.insert_values(software_version='6.2.2',\n",
    "                    operator_name='Xavier Mouy',\n",
    "                    audio_channel=deployment_info.audio_channel_number[0],\n",
    "                    UTC_offset=deployment_info.UTC_offset[0],\n",
    "                    audio_file_dir=os.path.join(root_dir, data_dir),\n",
    "                    audio_sampling_frequency=deployment_info.sampling_frequency[0],\n",
    "                    audio_bit_depth=deployment_info.bit_depth[0],\n",
    "                    mooring_platform_name=deployment_info.mooring_platform_name[0],\n",
    "                    recorder_type=deployment_info.recorder_type[0],\n",
    "                    recorder_SN=deployment_info.recorder_SN[0],\n",
    "                    hydrophone_model=deployment_info.hydrophone_model[0],\n",
    "                    hydrophone_SN=deployment_info.hydrophone_SN[0],\n",
    "                    hydrophone_depth=deployment_info.hydrophone_depth[0],\n",
    "                    location_name=deployment_info.location_name[0],\n",
    "                    location_lat=deployment_info.location_lat[0],\n",
    "                    location_lon=deployment_info.location_lon[0],\n",
    "                    location_water_depth=deployment_info.location_water_depth[0],\n",
    "                    deployment_ID=deployment_info.deployment_ID[0],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No inconsistent class labels this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FS']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class      FS  Total\n",
      "deployment_ID             \n",
      "ONC-Delta-2014  857    857\n",
      "Total           857    857\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the cleaned up dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as parquet file\n",
    "annot.to_parquet(os.path.join(root_dir, 'Annotations_dataset_' + deployment_info.deployment_ID[0] + '.parquet'))\n",
    "annot.to_pamlab(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +' annotations.log', single_file=True)\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4: UVIC - Hornby Island"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the step above for all the other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 annotation files found.\n",
      "Duplicate entries removed: 21162\n",
      "Integrity test succesfull\n",
      "21162 annotations imported.\n"
     ]
    }
   ],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\PhD\\Projects\\Dectector\\datasets\\UVIC_hornby-island_2019'\n",
    "deployment_file = r'deployment_info.csv' \n",
    "annotation_dir = r'manual_annotations'\n",
    "data_dir = r'audio_data'\n",
    "\n",
    "# Instantiate\n",
    "Deployment = DeploymentInfo()\n",
    "\n",
    "# write empty file to fill in (do once only)\n",
    "#Deployment.write_template(os.path.join(root_dir, deployment_file))\n",
    "\n",
    "# load deployment file\n",
    "deployment_info = Deployment.read(os.path.join(root_dir, deployment_file))\n",
    "\n",
    "# load all annotations\n",
    "annot = Annotation()\n",
    "annot.from_raven(os.path.join(root_dir, annotation_dir), verbose=True)\n",
    "\n",
    "# Mnaually fill in missing information\n",
    "annot.insert_values(software_version='1.5',\n",
    "                    operator_name='Emie Woodburn',\n",
    "                    audio_channel=deployment_info.audio_channel_number[0],\n",
    "                    UTC_offset=deployment_info.UTC_offset[0],\n",
    "                    audio_file_dir=os.path.join(root_dir, data_dir),\n",
    "                    audio_sampling_frequency=deployment_info.sampling_frequency[0],\n",
    "                    audio_bit_depth=deployment_info.bit_depth[0],\n",
    "                    mooring_platform_name=deployment_info.mooring_platform_name[0],\n",
    "                    recorder_type=deployment_info.recorder_type[0],\n",
    "                    recorder_SN=deployment_info.recorder_SN[0],\n",
    "                    hydrophone_model=deployment_info.hydrophone_model[0],\n",
    "                    hydrophone_SN=deployment_info.hydrophone_SN[0],\n",
    "                    hydrophone_depth=deployment_info.hydrophone_depth[0],\n",
    "                    location_name = deployment_info.location_name[0],\n",
    "                    location_lat = deployment_info.location_lat[0],\n",
    "                    location_lon = deployment_info.location_lon[0],\n",
    "                    location_water_depth = deployment_info.location_water_depth[0],\n",
    "                    deployment_ID=deployment_info.deployment_ID[0],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some inconsistent class labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FS', 'Seal', 'Unknown', nan, ' FS', 'KW', 'KW ', 'Seal\\\\', ' ', 'FSFS', 'Chirp', '  ']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing labels according to our naming convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FS', 'HS', 'UN', 'KW']\n"
     ]
    }
   ],
   "source": [
    "annot.data['label_class'].replace(to_replace=['FSFS',' FS'], value='FS', inplace=True)\n",
    "annot.data['label_class'].replace(to_replace=['KW '], value='KW', inplace=True)\n",
    "annot.data['label_class'].replace(to_replace=['Seal','Seal\\\\'], value='HS', inplace=True)\n",
    "annot.data['label_class'].replace(to_replace=['Unknown','Chirp',' ','  '], value='UN', inplace=True)\n",
    "annot.data['label_class'].dropna(axis=0, inplace=True)\n",
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class       FS  HS  KW  UN  Total\n",
      "deployment_ID                          \n",
      "07-HI          21002  33  27  93  21155\n",
      "Total          21002  33  27  93  21155\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the cleaned up dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as parquet file\n",
    "annot.to_parquet(os.path.join(root_dir, 'Annotations_dataset_' + deployment_info.deployment_ID[0] + '.parquet'))\n",
    "annot.to_pamlab(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +' annotations.log', single_file=True)\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 5: UVIC - Mill Bay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the step above for all the other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 annotation files found.\n",
      "Duplicate entries removed: 4058\n",
      "Integrity test succesfull\n",
      "4058 annotations imported.\n"
     ]
    }
   ],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\PhD\\Projects\\Dectector\\datasets\\UVIC_mill-bay_2019'\n",
    "deployment_file = r'deployment_info.csv' \n",
    "annotation_dir = r'manual_annotations'\n",
    "data_dir = r'audio_data'\n",
    "\n",
    "# Instantiate\n",
    "Deployment = DeploymentInfo()\n",
    "\n",
    "# write empty file to fill in (do once only)\n",
    "#Deployment.write_template(os.path.join(root_dir, deployment_file))\n",
    "\n",
    "# load deployment file\n",
    "deployment_info = Deployment.read(os.path.join(root_dir, deployment_file))\n",
    "\n",
    "# load all annotations\n",
    "annot = Annotation()\n",
    "annot.from_raven(os.path.join(root_dir, annotation_dir),\n",
    "                 class_header='Sound Type',\n",
    "                 verbose=True)\n",
    "\n",
    "# Mnaually fill in missing information\n",
    "annot.insert_values(software_version='1.5',\n",
    "                    operator_name='Courtney Evin',\n",
    "                    audio_channel=deployment_info.audio_channel_number[0],\n",
    "                    UTC_offset=deployment_info.UTC_offset[0],\n",
    "                    audio_file_dir=os.path.join(root_dir, data_dir),\n",
    "                    audio_sampling_frequency=deployment_info.sampling_frequency[0],\n",
    "                    audio_bit_depth=deployment_info.bit_depth[0],\n",
    "                    mooring_platform_name=deployment_info.mooring_platform_name[0],\n",
    "                    recorder_type=deployment_info.recorder_type[0],\n",
    "                    recorder_SN=deployment_info.recorder_SN[0],\n",
    "                    hydrophone_model=deployment_info.hydrophone_model[0],\n",
    "                    hydrophone_SN=deployment_info.hydrophone_SN[0],\n",
    "                    hydrophone_depth=deployment_info.hydrophone_depth[0],\n",
    "                    location_name = deployment_info.location_name[0],\n",
    "                    location_lat = deployment_info.location_lat[0],\n",
    "                    location_lon = deployment_info.location_lon[0],\n",
    "                    location_water_depth = deployment_info.location_water_depth[0],\n",
    "                    deployment_ID=deployment_info.deployment_ID[0],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some inconsistent class labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FS', 'HS', 'unknown-mammal?', 'unknown', nan, 'unknown-invert', 'fs', 'F', 'SF']\n"
     ]
    }
   ],
   "source": [
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing labels according to our naming convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FS', 'HS', 'UN']\n"
     ]
    }
   ],
   "source": [
    "annot.data['label_class'].replace(to_replace=['fs','F','SF'], value='FS', inplace=True)\n",
    "annot.data['label_class'].replace(to_replace=['unknown-mammal?','unknown','unknown-invert'], value='UN', inplace=True)\n",
    "annot.data['label_class'].dropna(axis=0, inplace=True)\n",
    "print(annot.get_labels_class())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class      FS  HS  UN  Total\n",
      "deployment_ID                     \n",
      "06-MILL        3987  49  17   4053\n",
      "Total          3987  49  17   4053\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the cleaned up dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as parquet file\n",
    "annot.to_parquet(os.path.join(root_dir, 'Annotations_dataset_' + deployment_info.deployment_ID[0] + '.parquet'))\n",
    "annot.to_pamlab(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +' annotations.log', single_file=True)\n",
    "annot.to_raven(root_dir, outfile='Annotations_dataset_' + deployment_info.deployment_ID[0] +'.Table.1.selections.txt', single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging all datasets together\n",
    "\n",
    "Now that all our datasets are cleaned up, we can merge them all in a single Master annotation dataset.\n",
    "\n",
    "Defining the path of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r'C:\\Users\\xavier.mouy\\Documents\\PhD\\Projects\\Dectector\\datasets'\n",
    "dataset_files = ['UVIC_mill-bay_2019\\Annotations_dataset_06-MILL.parquet',\n",
    "                 'UVIC_hornby-island_2019\\Annotations_dataset_07-HI.parquet',\n",
    "                 'ONC_delta-node_2014\\Annotations_dataset_ONC-Delta-2014.parquet',\n",
    "                 'DFO_snake-island_rca-in_20181017\\Annotations_dataset_SI-RCAIn-20181017.parquet',\n",
    "                 'DFO_snake-island_rca-out_20181015\\Annotations_dataset_SI-RCAOut-20181015.parquet',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through each dataset and merging in to a master dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "4058 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "21162 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "857 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "13016 annotations imported.\n",
      "Duplicate entries removed: 0\n",
      "Integrity test succesfull\n",
      "1932 annotations imported.\n"
     ]
    }
   ],
   "source": [
    "# # load all annotations\n",
    "annot = Annotation()\n",
    "for file in dataset_files:\n",
    "    tmp = Annotation()\n",
    "    tmp.from_parquet(os.path.join(root_dir, file), verbose=True)\n",
    "    annot = annot + tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a summary of all the annotatiosn we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class         ANT     FS  HS  KW   UN  Total\n",
      "deployment_ID                                     \n",
      "06-MILL               0   3987  49   0   17   4053\n",
      "07-HI                 0  21002  33  27   93  21155\n",
      "ONC-Delta-2014        0    857   0   0    0    857\n",
      "SI-RCAIn-20181017     2  12337   0   0  672  13011\n",
      "SI-RCAOut-20181015    0   1909   0   0   23   1932\n",
      "Total                 2  40092  82  27  805  41008\n"
     ]
    }
   ],
   "source": [
    "# print summary (pivot table)\n",
    "print(annot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the contribution from each analyst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_class       ANT     FS  HS  KW   UN  Total\n",
      "operator_name                                   \n",
      "Courtney Evin       0   3987  49   0   17   4053\n",
      "Emie Woodburn       0  21002  33  27   93  21155\n",
      "Stephanie Archer    2  14246   0   0  695  14943\n",
      "Xavier Mouy         0    857   0   0    0    857\n",
      "Total               2  40092  82  27  805  41008\n"
     ]
    }
   ],
   "source": [
    "print(annot.summary(rows='operator_name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can save our Master annotation dataset. It will be used for trainning and evealuation classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.to_parquet(os.path.join(root_dir, 'Master_annotations_dataset.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
